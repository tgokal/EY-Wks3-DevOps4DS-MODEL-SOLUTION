{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the experiment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1634798587211
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'driver-training/porto_seguro_safe_driver_prediction_input.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "training_folder = 'driver_training'\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "# Copy the data file into the experiment folder\n",
    "shutil.copy('data/porto_seguro_safe_driver_prediction_input.csv', os.path.join(training_folder, \"porto_seguro_safe_driver_prediction_input.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lightgbm==2.3.0'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\"lightgbm=={}\".format(lgb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py\n",
    "This file defines the key functions required to train the model.  \n",
    "The file can be invoked with `python train.py` for development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting driver-training/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/train.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm\n",
    "\n",
    "\n",
    "def split_data(data_df):\n",
    "    \"\"\"Split a dataframe into training and validation datasets\"\"\"\n",
    "    \n",
    "    features = data_df.drop(['target', 'id'], axis = 1)\n",
    "    labels = np.array(data_df['target'])\n",
    "    features_train, features_valid, labels_train, labels_valid = train_test_split(features, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "    train_data = lightgbm.Dataset(features_train, label=labels_train)\n",
    "    valid_data = lightgbm.Dataset(features_valid, label=labels_valid, free_raw_data=False)\n",
    "    \n",
    "    return (train_data, valid_data)\n",
    "\n",
    "\n",
    "def train_model(data, parameters):\n",
    "    \"\"\"Train a model with the given datasets and parameters\"\"\"\n",
    "    # The object returned by split_data is a tuple.\n",
    "    # Access train_data with data[0] and valid_data with data[1]\n",
    "    \n",
    "    model = lightgbm.train(parameters,\n",
    "                           data[0],\n",
    "                           valid_sets=data[1],\n",
    "                           num_boost_round=500,\n",
    "                           early_stopping_rounds=20)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_metrics(model, data):\n",
    "    \"\"\"Construct a dictionary of metrics for the model\"\"\"\n",
    "    \n",
    "    valid_data = data[1]\n",
    "    predictions = model.predict(valid_data.data)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(valid_data.label, predictions)\n",
    "    model_metrics = {\"auc\": (metrics.auc(fpr, tpr))}\n",
    "    \n",
    "    return model_metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"This method invokes the training functions for development purposes\"\"\"\n",
    "    \n",
    "    # Read data from a file\n",
    "    data_df = pd.read_csv('porto_seguro_safe_driver_prediction_input.csv')\n",
    "\n",
    "    # Hard code the parameters for training the model\n",
    "    parameters = {\n",
    "        'learning_rate': 0.02,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'sub_feature': 0.7,\n",
    "        'num_leaves': 60,\n",
    "        'min_data': 100,\n",
    "        'min_hessian': 1,\n",
    "        'verbose': 2\n",
    "    }\n",
    "\n",
    "    # Call the functions defined in this file\n",
    "    data = split_data(data_df)\n",
    "    model = train_model(data, parameters)\n",
    "    model_metrics = get_model_metrics(model, data)\n",
    "    \n",
    "    # Print the resulting metrics for the model\n",
    "    print(model_metrics)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters.json\n",
    "This file will specify the parameters used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting driver-training/parameters.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/parameters.json\n",
    "{\n",
    "    \"training\":\n",
    "    {\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"sub_feature\": 0.7,\n",
    "        \"num_leaves\": 60,\n",
    "        \"min_data\": 100,\n",
    "        \"min_hessian\": 1,\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## driver_training.py\n",
    "This file will be the entry script when running an Azure ML context.  \n",
    "It calls the functions defined in train.py for data preparation and training, but reads parameters from a file, and logs output to the Azure ML context.  \n",
    "The file can be invoked with `python driver_training.py` for development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting driver-training/driver_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/driver_training.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "from azureml.core import Run\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Import functions from train.py\n",
    "from train import split_data, train_model, get_model_metrics\n",
    "\n",
    "# Get the output folder for the model from the '--output_folder' parameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--output_folder', type=str, dest='output_folder', default=\"outputs\")\n",
    "args = parser.parse_args()\n",
    "output_folder = args.output_folder\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the safe driver prediction dataset\n",
    "train_df = pd.read_csv('porto_seguro_safe_driver_prediction_input.csv')\n",
    "\n",
    "# Load the parameters for training the model from the file\n",
    "with open(\"parameters.json\") as f:\n",
    "    pars = json.load(f)\n",
    "    parameters = pars[\"training\"]\n",
    "\n",
    "# Log each of the parameters to the run\n",
    "for param_name, param_value in parameters.items():\n",
    "    run.log(param_name, param_value)\n",
    "    \n",
    "# Use the functions imported from train.py to prepare data, train the model, and calculate the metrics\n",
    "data = split_data(train_df)\n",
    "model = train_model(data, parameters)\n",
    "model_metrics = get_model_metrics(model, data)\n",
    "print(model_metrics)\n",
    "run.log(\"model AUC\", model_metrics)\n",
    "\n",
    "# Save the trained model to the output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = output_folder + \"/porto_seguro_safe_driver_model.pkl\"\n",
    "joblib.dump(value=model, filename=output_path)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/porto_seguro_safe_driver_model.pkl\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"outputs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = output_folder + \"/porto_seguro_safe_driver_model.pkl\"\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1634798590606
    }
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "# Load the workspace\n",
    "ws = Workspace.from_config()\n",
    "exp = Experiment(ws, 'driver-training2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use an Estimator to Run the Script as an Experiment\n",
    "\n",
    "See [this tutorial](https://github.com/MicrosoftDocs/mslearn-aml-labs/blob/master/02-Training_Models.ipynb) for a starting point\n",
    "\n",
    "Use the pip, scikit-learn and lightgbm conda packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1634798592608
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "## Create compute cluster\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found existing compute target. Using this one. ' + compute_name)\n",
    "else:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes,\n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1634798592780
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210806.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"myenv\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"conda-forge\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.7\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"scikit-learn\",\n",
       "                        \"lightgbm\",\n",
       "                        \"pandas\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"name\": \"azureml_62b4299fade8de3e315f2be6e3e48acb\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"2\"\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Create an environment\n",
    "\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "conda = CondaDependencies()\n",
    "\n",
    "# add conda packages\n",
    "conda.add_conda_package('python=3.7')\n",
    "\n",
    "# add pip packages\n",
    "conda.add_pip_package('scikit-learn')\n",
    "conda.add_pip_package('lightgbm')\n",
    "conda.add_pip_package('pandas')\n",
    "# create environment\n",
    "env = Environment('myenv')\n",
    "env.python.conda_dependencies = conda\n",
    "env.register(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1634798852214
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Submitting /mnt/batch/tasks/shared/LS_root/mounts/clusters/tg-dev-box/code/Users/tgokal/devops-for-data-science-artifacts/Challenge02/driver-training directory for run. The size of the directory >= 25 MB, so it can take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import ScriptRunConfig, Experiment\n",
    "\n",
    "# create or load an experiment\n",
    "workspace = ws\n",
    "experiment = Experiment(workspace, 'drivertraining2')\n",
    "# create or retrieve a compute target\n",
    "cluster = workspace.compute_targets['cpu-cluster']\n",
    "# create or retrieve an environment\n",
    "env = Environment.get(ws, name='myenv')\n",
    "# define arguments\n",
    "arg=\"outputs\"\n",
    "# configure and submit your training run\n",
    "config = ScriptRunConfig(source_directory='driver-training',\n",
    "                        script='driver_training.py',\n",
    "                        arguments=['--output_folder', arg],\n",
    "                        compute_target=cluster,\n",
    "                        environment=env)\n",
    "\n",
    "run = experiment.submit(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate :: 0.02\n",
      "boosting_type :: gbdt\n",
      "objective :: binary\n",
      "metric :: auc\n",
      "sub_feature :: 0.7\n",
      "num_leaves :: 60\n",
      "min_data :: 100\n",
      "min_hessian :: 1\n",
      "verbose :: 0\n",
      "model AUC :: {'auc': 0.6388635476931616}\n"
     ]
    }
   ],
   "source": [
    "# Return model metrics\n",
    "metrics = run.get_metrics()\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(metric_name, '::', metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete experiment run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to ACI and test model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver-training version 1\n"
     ]
    }
   ],
   "source": [
    "model = ws.models['driver-training']\n",
    "print(model.name, 'version', model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver-service folder created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_name = 'driver-service'\n",
    "\n",
    "# Create a folder for the web service files\n",
    "experiment_folder = './' + folder_name\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(folder_name, 'folder created.')\n",
    "\n",
    "# Set path for scoring script\n",
    "script_file = os.path.join(experiment_folder,\"score_driver.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a scoring script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./driver-service/score_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_file\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the deployed model file and load it\n",
    "    model_path = Model.get_model_path('driver-training')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    # Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # Get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    # Get the corresponding classname for each prediction (0 or 1)\n",
    "    classnames = ['no claim', 'claim']\n",
    "    predicted_classes = []\n",
    "    probabilities = [prediction for prediction in predictions]\n",
    "    for probability in probabilities:\n",
    "        if probability < 0.5:\n",
    "            predicted_classes.append(classnames[0])\n",
    "        else:\n",
    "            predicted_classes.append(classnames[1])\n",
    "\n",
    "    # Return the predictions\n",
    "    return (probabilities, predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define container environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dependency info in ./driver-service/driver_env.yml\n",
      "# Conda environment specification. The dependencies defined in this file will\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\n",
      "\n",
      "# Details about the Conda environment file format:\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "    # Required packages for AzureML execution, history, and data preparation.\n",
      "  - azureml-defaults\n",
      "\n",
      "- scikit-learn\n",
      "- lightgbm\n",
      "- pandas\n",
      "channels:\n",
      "- anaconda\n",
      "- conda-forge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "# Add the dependencies for our model (AzureML defaults is already included)\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package('scikit-learn')\n",
    "myenv.add_conda_package('lightgbm')\n",
    "myenv.add_conda_package('pandas')\n",
    "\n",
    "# Save the environment config as a .yml file\n",
    "env_file = os.path.join(experiment_folder,\"driver_env.yml\")\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)\n",
    "\n",
    "# Print the .yml file\n",
    "with open(env_file,\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Inference Config and deploy webservice to ACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-10-23 12:15:37+00:00 Creating Container Registry if not exists.\n",
      "2021-10-23 12:15:38+00:00 Registering the environment.\n",
      "2021-10-23 12:15:40+00:00 Use the existing image.\n",
      "2021-10-23 12:15:40+00:00 Generating deployment configuration.\n",
      "2021-10-23 12:15:41+00:00 Submitting deployment to compute.\n",
      "2021-10-23 12:15:49+00:00 Checking the status of deployment driver-service7..\n",
      "2021-10-23 12:17:32+00:00 Checking the status of inference endpoint driver-service7.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "# Configure the scoring environment\n",
    "inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                   entry_script=script_file,\n",
    "                                   conda_file=env_file)\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "\n",
    "service_name = \"driver-service7\"\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthy\n",
      "2021-10-23T12:17:24,219281900+00:00 - gunicorn/run \n",
      "Dynamic Python package installation is disabled.\n",
      "Starting HTTP server\n",
      "2021-10-23T12:17:24,222092900+00:00 - iot-server/run \n",
      "2021-10-23T12:17:24,250303100+00:00 - rsyslog/run \n",
      "2021-10-23T12:17:24,258165100+00:00 - nginx/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-10-23T12:17:24,590908400+00:00 - iot-server/finish 1 0\n",
      "2021-10-23T12:17:24,593299700+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://127.0.0.1:31311 (74)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 100\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-10-23 12:17:26,951 | root | INFO | Starting up app insights client\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2021-10-23 12:17:26,956 | root | INFO | Starting up request id generator\n",
      "2021-10-23 12:17:26,956 | root | INFO | Starting up app insight hooks\n",
      "2021-10-23 12:17:26,956 | root | INFO | Invoking user's init function\n",
      "2021-10-23 12:17:28,147 | root | INFO | Users's init has completed successfully\n",
      "2021-10-23 12:17:28,157 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-10-23 12:17:28,157 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-10-23 12:17:28,158 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2021-10-23 12:17:32,204 | root | INFO | Swagger file not present\n",
      "2021-10-23 12:17:32,204 | root | INFO | 404\n",
      "127.0.0.1 - - [23/Oct/2021:12:17:32 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-10-23 12:17:37,027 | root | INFO | Swagger file not present\n",
      "2021-10-23 12:17:37,028 | root | INFO | 404\n",
      "127.0.0.1 - - [23/Oct/2021:12:17:37 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(service.state)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver-service7\n",
      "driver-service6\n",
      "driver-service5\n",
      "driver-service4\n",
      "driver-service3\n",
      "driver-service2\n",
      "driver-service1\n",
      "driver-service\n"
     ]
    }
   ],
   "source": [
    "for webservice_name in ws.webservices:\n",
    "    print(webservice_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Webservice for 1 driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient: [0, 1, 8, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 12, 1, 0, 0, 0.5, 0.3, 0.610327781, 7, 1, -1, 0, -1, 1, 1, 1, 2, 1, 65, 1, 0.316227766, 0.669556409, 0.352136337, 3.464101615, 0.1, 0.8, 0.6, 1, 1, 6, 3, 6, 2, 9, 1, 1, 1, 12, 0, 1, 1, 0, 0, 1]\n",
      "Probability of claim is: 0.029411417013210858\n",
      "Predicted class is:  ['no claim']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "x_new = [[0,1,8,1,0,0,1,0,0,0,0,0,0,0,12,1,0,0,0.5,0.3,0.610327781,7,1,-1,0,-1,1,1,1,2,1,65,1,0.316227766,0.669556409,0.352136337,3.464101615,0.1,0.8,0.6,1,1,6,3,6,2,9,1,1,1,12,0,1,1,0,0,1]]\n",
    "print ('Patient: {}'.format(x_new[0]))\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Call the web service, passing the input data (the web service will also accept the data in binary format)\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "print(\"Probability of claim is:\", predictions[0][0])\n",
    "print(\"Predicted class is: \", predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Webservice for multiple drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver 1 submits no claim with a probability of 0.0294\n",
      "Driver 2 submits no claim with a probability of 0.0259\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# This time our input is an array of two feature arrays\n",
    "x_new = [[0,1,8,1,0,0,1,0,0,0,0,0,0,0,12,1,0,0,0.5,0.3,0.610327781,7,1,-1,0,-1,1,1,1,2,1,65,1,0.316227766,0.669556409,0.352136337,3.464101615,0.1,0.8,0.6,1,1,6,3,6,2,9,1,1,1,12,0,1,1,0,0,1],\n",
    "         [4,2,5,1,0,0,0,0,1,0,0,0,0,0,5,1,0,0,0.9,0.5,0.771362431,4,1,-1,0,0,11,1,1,0,1,103,1,0.316227766,0.60632002,0.358329457,2.828427125,0.4,0.5,0.4,3,3,8,4,10,2,7,2,0,3,10,0,0,1,1,0,1]]\n",
    "# Convert the array or arrays to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Call the web service, passing the input data\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "# Get the predicted classes.\n",
    "for i in range(len(predictions)):\n",
    "    print(\"Driver {} submits {} with a probability of {}\".format(i+1, predictions[1][i], round(predictions[0][i], 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return scoring endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://054f4f39-3e63-4213-80d6-97f63469bf5f.australiaeast.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit POST Request to webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver 1 submits no claim with a probability of 0.0294\n",
      "Driver 2 submits no claim with a probability of 0.0259\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[0,1,8,1,0,0,1,0,0,0,0,0,0,0,12,1,0,0,0.5,0.3,0.610327781,7,1,-1,0,-1,1,1,1,2,1,65,1,0.316227766,0.669556409,0.352136337,3.464101615,0.1,0.8,0.6,1,1,6,3,6,2,9,1,1,1,12,0,1,1,0,0,1],[4,2,5,1,0,0,0,0,1,0,0,0,0,0,5,1,0,0,0.9,0.5,0.771362431,4,1,-1,0,0,11,1,1,0,1,103,1,0.316227766,0.60632002,0.358329457,2.828427125,0.4,0.5,0.4,3,3,8,4,10,2,7,2,0,3,10,0,0,1,1,0,1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "predictions = (requests.post(endpoint, input_json, headers = headers)).json()\n",
    "\n",
    "# Get the predicted classes.\n",
    "for i in range(len(predictions)):\n",
    "    print(\"Driver {} submits {} with a probability of {}\".format(i+1, predictions[1][i], round(predictions[0][i], 4)))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
